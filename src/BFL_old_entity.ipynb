{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate entity end dates: brownfield-land\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date**:  13 Jan 2025 <br>\n",
    "**Scope**: `dataset` <br>\n",
    "**Report Type**: Ad-hoc analysis <br>\n",
    "**Purpose**: Script to identify BFL entities which can be given an end-date on the basis that their references do not appear on the latest resource for a provision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "td = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "out_dir = \"../output/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "## Problem organisations to exclude\n",
    "# This is a list of organisations who we know supply multiple endpoints for multiple areas - we need to exclude these as this process only works for \n",
    "# provisions where there is one dataset updated over subsequent endpoints and resources, rather than multiple datasets.\n",
    "\n",
    "orgs_to_exclude = [\"local-authority:BUC\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasette_query(db, sql_string):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": sql_string,\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"https://datasette.planning.data.gov.uk/{db}.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def get_all_organisations():\n",
    "    q = \"\"\"\n",
    "        select organisation, name, entity as organisation_entity\n",
    "        from organisation\n",
    "        where end_date = \"\"\n",
    "        \"\"\"\n",
    "    return datasette_query(\"digital-land\", q)\n",
    "\n",
    "FILES_URL = 'https://datasette.planning.data.gov.uk/'\n",
    "\n",
    "def download_dataset(dataset, output_dir_path, overwrite=False):\n",
    "    dataset_file_name = f'{dataset}.db'\n",
    "    \n",
    "    if not os.path.exists(output_dir_path):\n",
    "        os.makedirs(output_dir_path)\n",
    "    \n",
    "    output_file_path = os.path.join(output_dir_path, dataset_file_name)\n",
    "\n",
    "    if overwrite is False and os.path.exists(output_file_path):\n",
    "        return\n",
    "    \n",
    "    final_url = os.path.join(FILES_URL, dataset_file_name)\n",
    "    print(f'downloading data from {final_url}')\n",
    "    print(f'to: {output_file_path}')\n",
    "    urllib.request.urlretrieve(final_url, os.path.join(output_dir_path, dataset_file_name))\n",
    "    print('download complete')\n",
    "\n",
    "def query_sqlite(db_path, query_string):\n",
    "\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "            \n",
    "        cursor = con.execute(query_string)\n",
    "        cols = [column[0] for column in cursor.description]\n",
    "        results_df = pd.DataFrame.from_records(data=cursor.fetchall(), columns=cols)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get historic endpoints and resources for a provision, along with incrementing resource key\n",
    "def get_historic_resources(dataset, organisation):\n",
    "\n",
    "    q = f\"\"\"\n",
    "\n",
    "        WITH endpoint_count as (\n",
    "            SELECT \n",
    "                distinct organisation,\n",
    "                latest_status, \n",
    "                endpoint, \n",
    "                endpoint_entry_date, \n",
    "                endpoint_end_date, \n",
    "                resource, \n",
    "                resource_start_date, \n",
    "                resource_end_date,\n",
    "                dense_rank() over (order by endpoint_entry_date, endpoint) as endpoint_no\n",
    "\n",
    "            FROM reporting_historic_endpoints\n",
    "            WHERE pipeline = '{dataset}'\n",
    "            AND organisation = '{organisation}'\n",
    "            AND (latest_status = 200 OR latest_status = \"\")\n",
    "            ORDER BY endpoint_entry_date, resource_start_date\n",
    "        ),\n",
    "\n",
    "        endpoint_resource_count as (\n",
    "            SELECT \n",
    "                *,\n",
    "                CAST(CAST(endpoint_no as string) || \".\" || dense_rank() over (partition by endpoint_no order by resource_start_date, latest_status) as float) as endpoint_resource_no\n",
    "            FROM endpoint_count\n",
    "        )\n",
    "\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE WHEN resource_end_date = \"\" then \n",
    "                LEAD(endpoint_entry_date) over (order by endpoint_resource_no) \n",
    "                ELSE resource_end_date end\n",
    "                as entity_end_date\n",
    "\n",
    "        FROM endpoint_resource_count\n",
    "\n",
    "\"\"\"\n",
    "    r = datasette_query(\"performance\", q)\n",
    "\n",
    "    return r \n",
    "\n",
    "# get_historic_resources(\"brownfield-land\", \"local-authority:BST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resource_references(resources, db_path):\n",
    "\n",
    "    # take list of resources and db path to get all reference values which have appeared on each resource\n",
    "\n",
    "    q = \"\"\" \n",
    "\n",
    "        SELECT distinct fr.resource, f.value \n",
    "        FROM fact_resource fr\n",
    "        INNER JOIN fact f on fr.fact = f.fact\n",
    "        WHERE 1=1\n",
    "            AND f.field = \"reference\"\n",
    "            AND fr.resource in ({})\n",
    "    \"\"\".format(', '.join(f\"'{r}'\" for r in resources))\n",
    "\n",
    "    df = query_sqlite(db_path, q)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download performance db\n",
    "download_dataset(\"brownfield-land\", data_dir, overwrite=True)\n",
    "bfl_path = os.path.join(data_dir, \"brownfield-land.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_lookup = get_all_organisations()\n",
    "\n",
    "org_dict = dict(zip(org_lookup[\"organisation\"], org_lookup[\"organisation_entity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_bfl_all = datasette_query(\n",
    "    \"digital-land\",\n",
    "    \"\"\"\n",
    "    SELECT organisation\n",
    "    FROM provision\n",
    "    WHERE dataset = \"brownfield-land\"\n",
    "    AND end_date = \"\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# remove organisations to exclude\n",
    "prov_bfl = prov_bfl_all[~prov_bfl_all[\"organisation\"].isin(orgs_to_exclude)].copy()\n",
    "\n",
    "print(len(prov_bfl_all))\n",
    "print(len(prov_bfl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_out = []\n",
    "\n",
    "for org in prov_bfl[\"organisation\"]:\n",
    "    \n",
    "    # get all valid resources\n",
    "    res_hist = get_historic_resources(\"brownfield-land\", org)\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(org)\n",
    "    print(f\"Number of endpoint & resource records found: {len(res_hist)}\")\n",
    "\n",
    "    if len(res_hist) > 1:\n",
    "\n",
    "        # get entities\n",
    "        org_entities = query_sqlite(bfl_path, \n",
    "            f\"\"\"\n",
    "                SELECT * \n",
    "                FROM entity\n",
    "                WHERE organisation_entity = {org_dict[org]}\n",
    "                and end_date = \"\"\n",
    "            \"\"\")\n",
    "\n",
    "        print(f\"Number of live entities: {len(org_entities)}\")\n",
    "\n",
    "        # get all reference values for resources\n",
    "        res_refs = get_resource_references(\n",
    "            res_hist[\"resource\"].drop_duplicates(), \n",
    "            bfl_path)\n",
    "        \n",
    "        #Â join resources to list of reference values per resource\n",
    "        org_res_refs = res_hist.merge(\n",
    "            res_refs,\n",
    "            how = \"left\",\n",
    "            on = \"resource\"\n",
    "        )\n",
    "\n",
    "        # get the endpoint-resource count code for the most recent resource each reference has appeared in \n",
    "        ref_latest_res = org_res_refs.groupby([\"value\"], as_index = False).agg(\n",
    "            endpoint_resource_no = (\"endpoint_resource_no\", \"max\")\n",
    "        )\n",
    "\n",
    "        print(len(ref_latest_res))\n",
    "        \n",
    "        # join back to the full resource table using the endpoint_resource_no to get the end-date of the most recent\n",
    "        ref_latest_res_dated = ref_latest_res.merge(\n",
    "            org_res_refs[[\"resource\", \"endpoint_resource_no\", \"value\", \"entity_end_date\"]],\n",
    "            how = \"left\",\n",
    "            on = [\"value\", \"endpoint_resource_no\"]\n",
    "        )\n",
    "\n",
    "        # join from entity table to our reference and resource calcs\n",
    "        entity_ref_latest_res_dated = org_entities[[\"entity\", \"organisation_entity\", \"reference\", \"end_date\"]].merge(\n",
    "            ref_latest_res_dated,\n",
    "            how = \"left\",\n",
    "            left_on = \"reference\",\n",
    "            right_on = \"value\"\n",
    "        )\n",
    "\n",
    "        entity_ref_latest_res_dated.drop(\"value\", axis=1, inplace=True)\n",
    "\n",
    "        # add flags for old and mystery entities\n",
    "        max_ep_res = org_res_refs[\"endpoint_resource_no\"].max()\n",
    "        entity_ref_latest_res_dated[\"old_entity\"] = np.where(entity_ref_latest_res_dated[\"endpoint_resource_no\"] != max_ep_res, True, False)\n",
    "        entity_ref_latest_res_dated[\"mystery_entity\"] = np.where(entity_ref_latest_res_dated[\"endpoint_resource_no\"].isnull(), True, False)\n",
    "\n",
    "        all_out.append(entity_ref_latest_res_dated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out_df = pd.concat(all_out)\n",
    "print(len(all_out_df))\n",
    "\n",
    "all_out_df[\"organisation_entity\"] = all_out_df[\"organisation_entity\"].astype(int)\n",
    "all_out_df = all_out_df.merge(\n",
    "    org_lookup[[\"organisation_entity\", \"organisation\"]],\n",
    "    on = \"organisation_entity\"\n",
    ")\n",
    "\n",
    "unique_ent_test = len(all_out_df) == len(all_out_df[\"entity\"].drop_duplicates())\n",
    "print(f\"Is the output table unique by entity: {unique_ent_test}\")\n",
    "\n",
    "# all_out_df[\"mystery_entity\"] = np.where(all_out_df[\"endpoint_resource_no\"].isnull(), True, False)\n",
    "# all_out_df[\"old_entity\"] = np.where(all_out_df[\"entity_end_date\"].notnull(), True, False)\n",
    "\n",
    "all_out_df.sort_values([\"organisation\", \"endpoint_resource_no\"], inplace=True, ascending= [True, False])\n",
    "\n",
    "all_out_df.to_csv(os.path.join(data_dir, f\"old_entity_results_all_{td}.csv\"), index = False)\n",
    "\n",
    "print(len(all_out_df))\n",
    "all_out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_out_df[all_out_df[\"old_entity\"]]))\n",
    "print(len(all_out_df[all_out_df[\"mystery_entity\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old entities are those flagged as old, and not flagged as mystery\n",
    "old_entities_out = all_out_df[\n",
    "    (all_out_df[\"old_entity\"]) & (~all_out_df[\"mystery_entity\"])][[\"entity\", \"entity_end_date\"]]\n",
    "\n",
    "old_entities_out.rename(columns={\"entity_end_date\":\"end-date\"}, inplace=True)\n",
    "\n",
    "old_entities_out.to_csv(os.path.join(out_dir, \"brownfield-land-end-date.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
